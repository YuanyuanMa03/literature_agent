#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¬¬äºŒé˜¶æ®µä¸»ç¨‹åº - PDFæ·±åº¦åˆ†ææµç¨‹

æµç¨‹ï¼š
1. è¯»å–ç¬¬ä¸€é˜¶æ®µç­›é€‰ç»“æœ
2. ä½¿ç”¨æ‰‹åŠ¨ä¸‹è½½çš„PDFæ–‡ä»¶ï¼ˆè·³è¿‡è‡ªåŠ¨ä¸‹è½½ï¼‰
3. PDFè½¬å›¾ç‰‡
4. DeepSeek-OCRè¯†åˆ«
5. Qwenæ·±åº¦åˆ†æ
6. æ±‡æ€»è¡¨æ ¼ã€å›¾åƒã€æ•°æ®é›†
"""

import os
import pandas as pd
from pathlib import Path
from dotenv import load_dotenv

from core.llm import LiteratureLLM
# from agents.pdf_download_agent import PDFDownloadAgent  # å·²ç¦ç”¨è‡ªåŠ¨ä¸‹è½½
from agents.deep_analysis_agent import DeepAnalysisAgent

load_dotenv()


def main():
    print("="*70)
    print("ğŸ”¬ æ–‡çŒ®æ·±åº¦åˆ†æç³»ç»Ÿ - Stage 2 (æ‰‹åŠ¨ä¸‹è½½æ¨¡å¼)")
    print("   ä½¿ç”¨æ‰‹åŠ¨ä¸‹è½½çš„PDF â†’ OCRè¯†åˆ« â†’ æ·±åº¦åˆ†æ")
    print("   ç»Ÿä¸€ä½¿ç”¨ç¡…åŸºæµåŠ¨API")
    print("="*70)
    
    # åˆå§‹åŒ–LLMï¼ˆQwenç”¨äºæ·±åº¦åˆ†æï¼ŒDeepSeek-OCRç”¨äºè¯†åˆ«ï¼‰
    # éƒ½ä½¿ç”¨ç›¸åŒçš„ç¡…åŸºæµåŠ¨APIé…ç½®
    llm = LiteratureLLM(
        model=os.getenv("LLM_MODEL", "Qwen/Qwen3-Omni-30B-A3B-Instruct"),
        temperature=0.1
    )
    
    # åˆå§‹åŒ–åˆ†æAgentï¼ˆä¸éœ€è¦ä¸‹è½½Agentï¼‰
    analysis_agent = DeepAnalysisAgent(
        llm,
        pdf_dir="./pdfs",
        image_dir="./pdf_images"
    )
    
    print("\nâœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    print("ğŸ“ PDFç›®å½•: ./pdfs/")
    print("ğŸ’¡ è¯·å°†æ‰‹åŠ¨ä¸‹è½½çš„PDFæ–‡ä»¶æ”¾å…¥ ./pdfs/ ç›®å½•\n")
    
    # è¯»å–ç¬¬ä¸€é˜¶æ®µç»“æœ
    try:
        screening_file = 'literature_screening_results.xlsx'
        df = pd.read_excel(screening_file)
        print(f"ğŸ“ è¯»å–ç­›é€‰ç»“æœ: {len(df)} ç¯‡æ–‡çŒ®")
        
        passed = (df['LLM_Decision'] == 'accept').sum()
        print(f"   é€šè¿‡ç­›é€‰: {passed} ç¯‡")
        
    except FileNotFoundError:
        print(f"âŒ æœªæ‰¾åˆ°ç­›é€‰ç»“æœæ–‡ä»¶: {screening_file}")
        print("   è¯·å…ˆè¿è¡Œ main.py å®Œæˆç¬¬ä¸€é˜¶æ®µç­›é€‰")
        return
    
    # ========== STAGE 2: æ·±åº¦åˆ†æ ==========
    
    # Step 1: æ‰«ææ‰‹åŠ¨ä¸‹è½½çš„PDFæ–‡ä»¶
    print("\n" + "="*70)
    print("ã€STAGE 2.1ã€‘æ‰«æPDFæ–‡ä»¶")
    print("="*70)
    
    pdf_dir = Path("./pdfs")
    pdf_files = list(pdf_dir.glob("*.pdf"))
    
    print(f"\nğŸ“‚ åœ¨ ./pdfs/ ç›®å½•ä¸­æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
    
    if len(pdf_files) == 0:
        print("\nâŒ æœªæ‰¾åˆ°ä»»ä½•PDFæ–‡ä»¶ï¼")
        print("\nè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š")
        print("1. æ‰‹åŠ¨ä¸‹è½½éœ€è¦åˆ†æçš„æ–‡çŒ®PDF")
        print("2. å°†PDFæ–‡ä»¶æ”¾å…¥ ./pdfs/ ç›®å½•")
        print("3. é‡æ–°è¿è¡Œæœ¬ç¨‹åº")
        print("\nğŸ’¡ æç¤ºï¼šæ–‡ä»¶åå¯ä»¥æ˜¯ä»»æ„æ ¼å¼ï¼ˆå¦‚ï¼špaper1.pdf, 10.1016_xxx.pdfç­‰ï¼‰")
        return
    
    # æ˜¾ç¤ºæ‰¾åˆ°çš„PDFæ–‡ä»¶
    print("\næ‰¾åˆ°çš„PDFæ–‡ä»¶ï¼š")
    for i, pdf_file in enumerate(pdf_files[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
        size_mb = pdf_file.stat().st_size / 1024 / 1024
        print(f"  {i}. {pdf_file.name} ({size_mb:.2f} MB)")
    
    if len(pdf_files) > 10:
        print(f"  ... è¿˜æœ‰ {len(pdf_files) - 10} ä¸ªæ–‡ä»¶")
    
    # ç”¨æˆ·é€‰æ‹©è¦åˆ†æçš„æ•°é‡
    print("\nè¯·é€‰æ‹©è¦åˆ†æçš„æ–‡çŒ®æ•°é‡ï¼š")
    print(f"1. æµ‹è¯•æ¨¡å¼ï¼ˆåˆ†æ3ç¯‡æ–‡çŒ®ï¼‰")
    print(f"2. å°æ‰¹é‡æ¨¡å¼ï¼ˆ10ç¯‡ï¼‰")
    print(f"3. å…¨éƒ¨æ–‡çŒ®ï¼ˆ{len(pdf_files)} ç¯‡ï¼‰")
    
    choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-3, é»˜è®¤1): ").strip() or "1"
    
    if choice == "1":
        max_papers = 3
        print(f"\nğŸ”¬ æµ‹è¯•æ¨¡å¼: å¤„ç† {min(max_papers, len(pdf_files))} ç¯‡")
    elif choice == "2":
        max_papers = 10
        print(f"\nğŸ”¬ å°æ‰¹é‡æ¨¡å¼: å¤„ç† {min(max_papers, len(pdf_files))} ç¯‡")
    else:
        max_papers = len(pdf_files)
        print(f"\nğŸ”¬ å®Œæ•´æ¨¡å¼: å¤„ç†å…¨éƒ¨ {len(pdf_files)} ç¯‡")
    
    # æ„é€ æ¨¡æ‹Ÿçš„download_resultsï¼ˆç”¨äºåˆ†æAgentï¼‰
    download_results = {
        'downloaded': min(max_papers, len(pdf_files)),
        'failed': 0,
        'total': len(pdf_files),
        'results': {}
    }
    
    # ä¸ºæ¯ä¸ªPDFæ–‡ä»¶åˆ›å»ºç»“æœæ¡ç›®
    for pdf_file in pdf_files[:max_papers]:
        # å°è¯•ä»æ–‡ä»¶åæå–DOIï¼ˆå¦‚æœæ–‡ä»¶åæ˜¯DOIæ ¼å¼ï¼‰
        doi = pdf_file.stem.replace('_', '/')
        download_results['results'][doi] = {
            'status': 'exists',
            'path': str(pdf_file),
            'source': 'manual'
        }
    
    print(f"âœ… å‡†å¤‡åˆ†æ {download_results['downloaded']} ç¯‡æ–‡çŒ®\n")
    
    # Step 2: æ·±åº¦åˆ†æ
    print("\n" + "="*70)
    print("ã€STAGE 2.2ã€‘æ·±åº¦åˆ†æ (PDFâ†’OCRâ†’åˆ†æ)")
    print("="*70)
    
    analysis_df = analysis_agent.run(
        download_results=download_results,
        max_papers=max_papers or 1000
    )
    
    # æœ€ç»ˆæ€»ç»“
    print("\n" + "="*70)
    print("ğŸ“Š ç¬¬äºŒé˜¶æ®µå®Œæˆç»Ÿè®¡")
    print("="*70)
    
    print(f"\nPDFæ–‡ä»¶:")
    print(f"  æœ¬åœ°æ–‡ä»¶: {download_results.get('downloaded', 0)} ç¯‡")
    
    if not analysis_df.empty:
        print(f"\næ·±åº¦åˆ†æ:")
        print(f"  æˆåŠŸåˆ†æ: {len(analysis_df)} ç¯‡")
        print(f"  æå–è¡¨æ ¼: {analysis_df['Tables_Count'].sum()} ä¸ª")
        print(f"  æå–å›¾è¡¨: {analysis_df['Figures_Count'].sum()} ä¸ª")
        
        # ç»Ÿè®¡æœ‰æ•°æ®é›†çš„æ–‡çŒ®
        has_datasets = sum(1 for x in analysis_df['Key_Datasets'] 
                          if x and x != '[]')
        print(f"  åŒ…å«æ•°æ®é›†: {has_datasets} ç¯‡")
    
    print(f"\nç”Ÿæˆæ–‡ä»¶:")
    print(f"  1. deep_analysis_results.xlsx - æ·±åº¦åˆ†æç»“æœ")
    print(f"  2. deep_analysis_results.json - è¯¦ç»†JSONç»“æœ")
    print(f"  3. ./pdfs/ - æºPDFæ–‡ä»¶")
    print(f"  4. ./pdf_images/ - PDFè½¬æ¢çš„å›¾ç‰‡")
    
    print("\n" + "="*70)
    print("âœ… ç¬¬äºŒé˜¶æ®µåˆ†æå…¨éƒ¨å®Œæˆï¼")
    print("="*70)


if __name__ == "__main__":
    main()
