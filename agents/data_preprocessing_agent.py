#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é¢„å¤„ç†Agent - å°†åŸå§‹WOSæ–‡çŒ®æ•°æ®å¤„ç†æˆç¬¦åˆagentåˆ†æçš„æ ‡å‡†æ ¼å¼
"""

import pandas as pd
from pathlib import Path
from typing import List, Dict, Optional, Union
from core.agent import Agent
from core.message import Message
import os


class DataPreprocessingAgent(Agent):
    """æ•°æ®é¢„å¤„ç†Agent"""
    
    def __init__(self, llm):
        """åˆå§‹åŒ–æ•°æ®é¢„å¤„ç†Agent"""
        super().__init__(
            name="æ•°æ®é¢„å¤„ç†Agent",
            llm=llm,
            system_prompt=self._default_system_prompt()
        )
    
    def _default_system_prompt(self) -> str:
        """é»˜è®¤ç³»ç»Ÿæç¤ºè¯"""
        return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡çŒ®æ•°æ®é¢„å¤„ç†ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š

1. å¤„ç†åŸå§‹Web of Science (WOS)å¯¼å‡ºçš„æ–‡çŒ®æ•°æ®
2. æå–å…³é”®ä¿¡æ¯ï¼šä½œè€…ã€å¹´ä»½ã€æ ‡é¢˜ã€æ‘˜è¦ã€DOI
3. æ¸…ç†å’Œæ ‡å‡†åŒ–æ•°æ®æ ¼å¼
4. ç”Ÿæˆç¬¦åˆagentåˆ†æéœ€æ±‚çš„æ ‡å‡†CSVæ ¼å¼
5. æä¾›æ•°æ®è´¨é‡ç»Ÿè®¡å’Œé¢„è§ˆ

ä½ éœ€è¦ç¡®ä¿æ•°æ®çš„å®Œæ•´æ€§ã€ä¸€è‡´æ€§å’Œå¯ç”¨æ€§ã€‚"""
    
    def run(
        self,
        input_source: Union[str, List[str], pd.DataFrame] = "auto",
        output_dir: str = "data",
        output_filename: str = "literature_data_processed.csv",
        auto_detect: bool = True
    ) -> Dict[str, any]:
        """
        è¿è¡Œæ•°æ®é¢„å¤„ç†æµç¨‹
        
        Parameters:
        -----------
        input_source : Union[str, List[str], pd.DataFrame]
            è¾“å…¥æºï¼š
            - "auto": è‡ªåŠ¨æ£€æµ‹å½“å‰ç›®å½•ä¸‹çš„Excelæ–‡ä»¶
            - str: å•ä¸ªæ–‡ä»¶è·¯å¾„
            - List[str]: å¤šä¸ªæ–‡ä»¶è·¯å¾„åˆ—è¡¨
            - pd.DataFrame: ç›´æ¥è¾“å…¥DataFrame
        output_dir : str
            è¾“å‡ºç›®å½•
        output_filename : str
            è¾“å‡ºæ–‡ä»¶å
        auto_detect : bool
            æ˜¯å¦è‡ªåŠ¨æ£€æµ‹WOSåˆ†æ®µæ–‡ä»¶
            
        Returns:
        --------
        Dict[str, any]: å¤„ç†ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
        """
        
        print("="*70)
        print("ğŸ”§ æ•°æ®é¢„å¤„ç†Agent - å¼€å§‹å¤„ç†æ–‡çŒ®æ•°æ®")
        print("="*70)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        try:
            # STEP 1: åŠ è½½åŸå§‹æ•°æ®
            print("\nã€STEP 1/4ã€‘åŠ è½½åŸå§‹æ•°æ®")
            df = self._load_data(input_source, auto_detect)
            
            # STEP 2: æå–å…³é”®ä¿¡æ¯
            print("\nã€STEP 2/4ã€‘æå–å…³é”®ä¿¡æ¯")
            df_processed = self._extract_key_info(df)
            
            # STEP 3: æ•°æ®æ¸…ç†å’Œæ ‡å‡†åŒ–
            print("\nã€STEP 3/4ã€‘æ•°æ®æ¸…ç†å’Œæ ‡å‡†åŒ–")
            df_cleaned = self._clean_and_standardize(df_processed)
            
            # STEP 4: ä¿å­˜ç»“æœ
            print("\nã€STEP 4/4ã€‘ä¿å­˜å¤„ç†ç»“æœ")
            output_file = self._save_results(
                df_cleaned,
                output_path / output_filename
            )
            
            # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
            stats = self._generate_statistics(df_cleaned)
            
            print("\n" + "="*70)
            print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼")
            print("="*70)
            
            return {
                "success": True,
                "output_file": str(output_file),
                "total_records": len(df_cleaned),
                "statistics": stats,
                "data_columns": list(df_cleaned.columns),
                "preview": df_cleaned.head(3).to_dict('records') if len(df_cleaned) > 0 else []
            }
            
        except Exception as e:
            error_msg = f"æ•°æ®é¢„å¤„ç†å¤±è´¥: {str(e)}"
            print(f"\nâŒ {error_msg}")
            return {
                "success": False,
                "error": error_msg,
                "output_file": None,
                "total_records": 0,
                "statistics": {},
                "data_columns": [],
                "preview": []
            }
    
    def _load_data(
        self,
        input_source: Union[str, List[str], pd.DataFrame],
        auto_detect: bool
    ) -> pd.DataFrame:
        """åŠ è½½åŸå§‹æ•°æ®"""
        
        if isinstance(input_source, pd.DataFrame):
            print("  âœ… ç›´æ¥ä½¿ç”¨è¾“å…¥çš„DataFrame")
            return input_source
        
        if input_source == "auto" and auto_detect:
            # è‡ªåŠ¨æ£€æµ‹WOSåˆ†æ®µæ–‡ä»¶
            wos_patterns = [
                '0-1000.xls', '1001-2000.xls', '2001-3000.xls', '3000-3674.xls',
                '0-1000.xlsx', '1001-2000.xlsx', '2001-3000.xlsx', '3000-3674.xlsx'
            ]
            
            found_files = []
            for pattern in wos_patterns:
                if Path(pattern).exists():
                    found_files.append(pattern)
            
            if found_files:
                print(f"  ğŸ” è‡ªåŠ¨æ£€æµ‹åˆ° {len(found_files)} ä¸ªWOSåˆ†æ®µæ–‡ä»¶:")
                for f in found_files:
                    print(f"    - {f}")
                input_files = found_files
            else:
                # æ£€æµ‹å…¶ä»–Excelæ–‡ä»¶
                excel_files = list(Path('.').glob('*.xls*'))
                if excel_files:
                    print(f"  ğŸ” æ£€æµ‹åˆ° {len(excel_files)} ä¸ªExcelæ–‡ä»¶:")
                    for f in excel_files:
                        print(f"    - {f}")
                    input_files = [str(f) for f in excel_files]
                else:
                    raise FileNotFoundError("æœªæ‰¾åˆ°ä»»ä½•Excelæ–‡ä»¶")
        elif isinstance(input_source, str):
            input_files = [input_source]
        else:
            input_files = input_source
        
        # è¯»å–å¹¶åˆå¹¶æ‰€æœ‰æ–‡ä»¶
        print(f"\n  ğŸ“– æ­£åœ¨è¯»å– {len(input_files)} ä¸ªæ–‡ä»¶...")
        dfs = []
        total_records = 0
        
        for input_file in input_files:
            try:
                print(f"    - è¯»å–: {input_file}")
                if input_file.endswith('.csv'):
                    df_temp = pd.read_csv(input_file)
                else:
                    df_temp = pd.read_excel(input_file)
                
                dfs.append(df_temp)
                file_records = len(df_temp)
                total_records += file_records
                print(f"      âœ… {file_records} ç¯‡æ–‡çŒ®")
                
            except Exception as e:
                print(f"      âŒ è¯»å–å¤±è´¥: {e}")
                continue
        
        if not dfs:
            raise ValueError("æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•æ–‡ä»¶")
        
        df = pd.concat(dfs, ignore_index=True)
        print(f"\n  âœ… åˆå¹¶å®Œæˆ: æ€»è®¡ {len(df)} ç¯‡æ–‡çŒ®")
        
        return df
    
    def _extract_key_info(self, df: pd.DataFrame) -> pd.DataFrame:
        """æå–å…³é”®ä¿¡æ¯"""
        
        print("  ğŸ” æå–å…³é”®åˆ—...")
        
        # å®šä¹‰å…³é”®åˆ—çš„å¤šç§å¯èƒ½åç§°
        column_mappings = {
            'authors': ['Authors', 'Author(s)', 'Author Full Names', 'ä½œè€…', 'ä½œè€…/Authors'],
            'year': ['Publication Year', 'Year', 'Publication Year', 'å¹´ä»½', 'å¹´ä»½/Year'],
            'title': ['Article Title', 'Title', 'Document Title', 'æ ‡é¢˜', 'æ ‡é¢˜/Title'],
            'abstract': ['Abstract', 'Abstract', 'æ‘˜è¦', 'æ‘˜è¦/Abstract'],
            'doi': ['DOI', 'DOI', 'DOI']
        }
        
        # æ‰¾åˆ°å®é™…å­˜åœ¨çš„åˆ—
        found_columns = {}
        for key, possible_names in column_mappings.items():
            for name in possible_names:
                if name in df.columns:
                    found_columns[key] = name
                    break
        
        if not found_columns:
            raise ValueError("æœªæ‰¾åˆ°ä»»ä½•æ ‡å‡†åˆ—ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼")
        
        print(f"    âœ… æ‰¾åˆ°å…³é”®åˆ—: {list(found_columns.keys())}")
        
        # æå–å…³é”®åˆ—
        key_columns = list(found_columns.values())
        df_key = df[key_columns].copy()
        
        # é‡å‘½åä¸ºæ ‡å‡†åç§°
        df_key.columns = list(found_columns.keys())
        
        return df_key
    
    def _clean_and_standardize(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…ç†å’Œæ ‡å‡†åŒ–æ•°æ®"""
        
        print("  ğŸ§¹ æ¸…ç†å’Œæ ‡å‡†åŒ–æ•°æ®...")
        
        original_count = len(df)
        
        # ç§»é™¤å®Œå…¨é‡å¤çš„è¡Œ
        df = df.drop_duplicates()
        duplicate_count = original_count - len(df)
        if duplicate_count > 0:
            print(f"    - ç§»é™¤é‡å¤æ–‡çŒ®: {duplicate_count} ç¯‡")
        
        # ç§»é™¤æ²¡æœ‰æ ‡é¢˜çš„è®°å½•
        if 'title' in df.columns:
            no_title_count = df['title'].isna().sum()
            if no_title_count > 0:
                df = df[df['title'].notna()]
                print(f"    - ç§»é™¤æ— æ ‡é¢˜æ–‡çŒ®: {no_title_count} ç¯‡")
        
        # æ ‡å‡†åŒ–å¹´ä»½æ ¼å¼
        if 'year' in df.columns:
            df['year'] = pd.to_numeric(df['year'], errors='coerce')
            invalid_year_count = df['year'].isna().sum()
            if invalid_year_count > 0:
                df = df[df['year'].notna()]
                print(f"    - ç§»é™¤æ— æ•ˆå¹´ä»½: {invalid_year_count} ç¯‡")
        
        # æŒ‰å¹´ä»½é™åºæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        if 'year' in df.columns:
            df = df.sort_values('year', ascending=False)
        
        # é‡ç½®ç´¢å¼•
        df = df.reset_index(drop=True)
        
        print(f"    âœ… æ¸…ç†å®Œæˆ: å‰©ä½™ {len(df)} ç¯‡æ–‡çŒ®")
        
        return df
    
    def _save_results(self, df: pd.DataFrame, output_file: Path) -> Path:
        """ä¿å­˜å¤„ç†ç»“æœ"""
        
        # é‡å‘½åä¸ºä¸­è‹±æ–‡å¯¹ç…§æ ¼å¼ï¼ˆä¿æŒä¸åŸç³»ç»Ÿå…¼å®¹ï¼‰
        column_mapping = {
            'authors': 'ä½œè€…/Authors',
            'year': 'å¹´ä»½/Year', 
            'title': 'æ ‡é¢˜/Title',
            'abstract': 'æ‘˜è¦/Abstract',
            'doi': 'DOI'
        }
        
        df_output = df.copy()
        df_output.columns = [column_mapping.get(col, col) for col in df.columns]
        
        # ä¿å­˜ä¸ºCSVï¼ˆå¸¦åºå·ï¼‰
        df_output.to_csv(
            output_file,
            index_label='åºå·',
            encoding='utf-8-sig'
        )
        
        # åŒæ—¶ä¿å­˜ä¸ºExcelæ ¼å¼
        excel_file = output_file.with_suffix('.xlsx')
        df_output.to_excel(excel_file, index=False)
        
        print(f"    âœ… CSVæ–‡ä»¶å·²ä¿å­˜: {output_file}")
        print(f"    âœ… Excelæ–‡ä»¶å·²ä¿å­˜: {excel_file}")
        
        return output_file
    
    def _generate_statistics(self, df: pd.DataFrame) -> Dict[str, any]:
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        
        print("  ğŸ“Š ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯...")
        
        stats = {
            "total_records": len(df),
            "columns": list(df.columns)
        }
        
        # å¹´ä»½ç»Ÿè®¡
        if 'year' in df.columns:
            year_range = f"{int(df['year'].min())} - {int(df['year'].max())}"
            stats["year_range"] = year_range
            
            # å¹´ä»½åˆ†å¸ƒï¼ˆå‰10ï¼‰
            year_counts = df['year'].value_counts().head(10).to_dict()
            stats["year_distribution"] = {int(k): v for k, v in year_counts.items()}
        
        # DOIç»Ÿè®¡
        if 'doi' in df.columns:
            doi_count = df['doi'].notna().sum()
            stats["doi_count"] = doi_count
            stats["doi_percentage"] = round(doi_count / len(df) * 100, 1)
        
        # æ‘˜è¦ç»Ÿè®¡
        if 'abstract' in df.columns:
            abstract_count = df['abstract'].notna().sum()
            stats["abstract_count"] = abstract_count
            stats["abstract_percentage"] = round(abstract_count / len(df) * 100, 1)
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print(f"\n  ğŸ“ˆ ã€ç»Ÿè®¡æ‘˜è¦ã€‘")
        print(f"    æ€»æ–‡çŒ®æ•°: {stats['total_records']}")
        if 'year_range' in stats:
            print(f"    å¹´ä»½èŒƒå›´: {stats['year_range']}")
        if 'doi_count' in stats:
            print(f"    æœ‰DOIçš„æ–‡çŒ®: {stats['doi_count']} ç¯‡ ({stats['doi_percentage']}%)")
        if 'abstract_count' in stats:
            print(f"    æœ‰æ‘˜è¦çš„æ–‡çŒ®: {stats['abstract_count']} ç¯‡ ({stats['abstract_percentage']}%)")
        
        return stats
    
    def preview_data(self, df: pd.DataFrame, n: int = 5) -> None:
        """é¢„è§ˆæ•°æ®"""
        
        print(f"\n  ğŸ‘€ ã€å‰{n}ç¯‡æ–‡çŒ®é¢„è§ˆã€‘")
        print("-" * 70)
        
        for i, row in df.head(n).iterrows():
            print(f"\n{i+1}. {row.get('authors', 'N/A')} ({int(row.get('year', 0))})")
            title = str(row.get('title', 'N/A'))
            print(f"   {title[:80]}{'...' if len(title) > 80 else ''}")
            
            if 'abstract' in row and pd.notna(row['abstract']):
                abstract = str(row['abstract'])
                print(f"   æ‘˜è¦: {abstract[:100]}{'...' if len(abstract) > 100 else ''}")
            
            doi = row.get('doi', 'N/A')
            print(f"   DOI: {doi if pd.notna(doi) else 'N/A'}")


# ä¾¿æ·å‡½æ•°ï¼Œä¿æŒä¸åŸè„šæœ¬çš„å…¼å®¹æ€§
def preprocess_wos_data(
    input_source: Union[str, List[str], pd.DataFrame] = "auto",
    output_dir: str = "data",
    output_filename: str = "literature_data_processed.csv"
) -> Dict[str, any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šé¢„å¤„ç†WOSæ•°æ®
    
    Parameters:
    -----------
    input_source : Union[str, List[str], pd.DataFrame]
        è¾“å…¥æº
    output_dir : str
        è¾“å‡ºç›®å½•
    output_filename : str
        è¾“å‡ºæ–‡ä»¶å
        
    Returns:
    --------
    Dict[str, any]: å¤„ç†ç»“æœ
    """
    
    from core.llm import LiteratureLLM
    
    # åˆ›å»ºLLMå®ä¾‹
    llm = LiteratureLLM()
    
    # åˆ›å»ºæ•°æ®é¢„å¤„ç†Agent
    agent = DataPreprocessingAgent(llm)
    
    # è¿è¡Œé¢„å¤„ç†
    return agent.run(
        input_source=input_source,
        output_dir=output_dir,
        output_filename=output_filename
    )


if __name__ == '__main__':
    # ç›´æ¥è¿è¡Œæ—¶çš„æµ‹è¯•ä»£ç 
    result = preprocess_wos_data()
    
    if result["success"]:
        print(f"\nğŸ‰ é¢„å¤„ç†æˆåŠŸï¼")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {result['output_file']}")
        print(f"ğŸ“Š å¤„ç†è®°å½•: {result['total_records']} ç¯‡")
    else:
        print(f"\nâŒ é¢„å¤„ç†å¤±è´¥: {result['error']}")